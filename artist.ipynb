{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5b1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56f6c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00abb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c0b02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line to prevent some possible errors\n",
    "!pip install --upgrade --no-cache-dir gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb8dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line to download the dataset if you haven't already\n",
    "!gdown 1-0d315aj7Ai8NNqat65XDvaOcHDcHiUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "190b39d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line to unzip the dataset if you didn't do it before\n",
    "!unzip famous_paintings.zip > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d87a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('data/*')\n",
    "df_train = pd.DataFrame({'full_path': files})\n",
    "files= [os.path.basename(f)for f in files]\n",
    "painters = [re.sub(r'_\\d+\\.jpg$', '', os.path.basename(f)) for f in files]\n",
    "\n",
    "  # TODO: get the painters' names from the file names\n",
    "df_train['painter'] = painters\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "929546a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb668a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of unique painters in the dataset\n",
    "class_names = df_train.painter.unique()\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b70a5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(15, 15))\n",
    "random_indices = np.random.choice(df_train.index, size=12, replace=False)\n",
    "for i, ax in zip(random_indices, axes.flatten()):\n",
    "    img = keras.preprocessing.image.load_img(df_train.full_path[i], target_size=(224, 224))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(df_train.painter[i])\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f80859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_train['label'] = le.fit_transform(df_train['painter'])# TODO: encode the painters' names\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7df507",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train['label'].unique().min())\n",
    "print(df_train['label'].unique().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b88d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# create a folder for each label\n",
    "for label in df_train.label.unique():\n",
    "    os.makedirs(f'data/{label}', exist_ok=True)\n",
    "\n",
    "# move each image to its corresponding label folder\n",
    "for i, row in df_train.iterrows():\n",
    "    shutil.move(row.full_path, f'data/{row.label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af3f7563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import image_dataset_from_directory\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    directory = './data/', # TODO\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'categorical',\n",
    "    color_mode = 'rgb',# TODO\n",
    "    batch_size=32, # TODO\n",
    "    image_size=(224, 224), # TODO\n",
    "    shuffle=True,\n",
    "    validation_split=0.2,\n",
    "    subset='training',# TODO\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# find the class names so in prediction time we can map the predictions to the painters properly\n",
    "class_names = train_dataset.class_names\n",
    "# print('Class names:', class_names)\n",
    "\n",
    "val_dataset = image_dataset_from_directory(\n",
    "    directory = './data/', # TODO\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'categorical',\n",
    "    color_mode = 'rgb',# TODO\n",
    "    batch_size=32, # TODO\n",
    "    image_size=(224, 224), # TODO\n",
    "    shuffle=False,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',# TODO\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ed4f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input\n",
    "train_dataset = train_dataset.map(lambda x, y: (preprocess_input(x), y))\n",
    "# Preprocess the data\n",
    "val_dataset = val_dataset.map(lambda x,y:(preprocess_input(x),y)) # TODO: apply the preprocess_input function to the val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1cc2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# metric: 'accuracy' or 'loss'\n",
    "def display_curves(history, metric):\n",
    "  df = pd.DataFrame(history.history[metric], columns=[metric])\n",
    "  df['val_'+metric] = history.history['val_'+metric]\n",
    "  fig = px.line(df, x= df.index+1, y= [metric, 'val_'+metric])\n",
    "  fig.update_layout(xaxis_title='Epochs', yaxis_title=metric)\n",
    "  fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58be4be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import RandomBrightness, RandomFlip, RandomRotation, RandomZoom, RandomContrast\n",
    "\n",
    "def build_model(use_pretrained=True, fine_tune_layers=None, use_data_augmentation=False, input_shape=(224, 224, 3), num_classes=10):\n",
    "    # Data Augmentation\n",
    "    if use_data_augmentation:\n",
    "        data_augmentation = keras.Sequential([\n",
    "            RandomBrightness(0.3),\n",
    "            RandomFlip(\"horizontal\"),\n",
    "            RandomRotation(0.2),\n",
    "            RandomZoom(0.1),\n",
    "            RandomContrast(0.1)\n",
    "        ])\n",
    "    else:\n",
    "        data_augmentation = None\n",
    "\n",
    "    # Base Model\n",
    "    base_model = keras.applications.ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet' if use_pretrained else None,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    if fine_tune_layers is None:\n",
    "        base_model.trainable = True\n",
    "    else:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[-fine_tune_layers:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    # Classification Head\n",
    "    x = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.Dense(128, activation='relu',keras.regularizers.l1_l2)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    output = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=base_model.input, outputs=output)\n",
    "    return model, data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ed3b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import RandomBrightness, RandomFlip, RandomRotation, RandomZoom, RandomContrast\n",
    "\n",
    "def build_model(use_pretrained=True, fine_tune_layers=None, use_data_augmentation=False, input_shape=(224, 224, 3), num_classes=10):\n",
    "    # Data Augmentation\n",
    "    if use_data_augmentation:\n",
    "        data_augmentation = keras.Sequential([\n",
    "            RandomBrightness(0.3),\n",
    "            RandomFlip(\"horizontal\"),\n",
    "            RandomRotation(0.2),\n",
    "            RandomZoom(0.1),\n",
    "            RandomContrast(0.1)\n",
    "        ])\n",
    "    else:\n",
    "        data_augmentation = None\n",
    "\n",
    "    # Base Model\n",
    "    base_model = keras.applications.ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet' if use_pretrained else None,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    if fine_tune_layers is None:\n",
    "        base_model.trainable = True\n",
    "    else:\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in base_model.layers[-fine_tune_layers:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    # Classification Head\n",
    "    x = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    x = keras.layers.Dense(256, activation='relu',kernel_regularizer=keras.regularizers.l1_l2(l1=1e-4, l2=1e-5))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.Dense(128, activation='relu',kernel_regularizer=keras.regularizers.l1_l2(l1=1e-4, l2=1e-5))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    output = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=base_model.input, outputs=output)\n",
    "    return model, data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51782cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model, data_aug = build_model(\n",
    "    use_pretrained=True,\n",
    "    fine_tune_layers=30,\n",
    "    use_data_augmentation=True,\n",
    "    num_classes=len(class_names)\n",
    ")\n",
    "\n",
    "# # Apply Data Augmentation\n",
    "# if data_aug:\n",
    "#     train_dataset = train_dataset.map(lambda x, y: (data_aug(x, training=True), y))\n",
    "\n",
    "# Compile\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "history = model.fit(train_dataset, epochs=30, validation_data=val_dataset, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a914edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model, data_aug = build_model(\n",
    "    use_pretrained=True,\n",
    "    fine_tune_layers=30,\n",
    "    use_data_augmentation=True,\n",
    "    num_classes=len(class_names)\n",
    ")\n",
    "\n",
    "# # Apply Data Augmentation\n",
    "# if data_aug:\n",
    "#     train_dataset = train_dataset.map(lambda x, y: (data_aug(x, training=True), y))\n",
    "\n",
    "# Compile\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "history = model.fit(train_dataset, epochs=5, validation_data=val_dataset, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecda3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_curves(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f64b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_curves(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a8ad01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ارزیابی روی داده‌های اعتبارسنجی\n",
    "val_loss, val_accuracy = model.evaluate(val_dataset)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    directory = './test_data',\n",
    "    labels = None,\n",
    "    image_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    shuffle=False\n",
    ")\n",
    "test_dataset = test_dataset.map(lambda x: preprocess_input(x))\n",
    "\n",
    "test_predictions = model.predict(test_dataset)\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec3e1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model, data_aug = build_model(\n",
    "    use_pretrained=True,\n",
    "    fine_tune_layers=30,\n",
    "    use_data_augmentation=True,\n",
    "    num_classes=len(class_names)\n",
    ")\n",
    "\n",
    "# # Apply Data Augmentation\n",
    "# if data_aug:\n",
    "#     train_dataset = train_dataset.map(lambda x, y: (data_aug(x, training=True), y))\n",
    "\n",
    "# Compile\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6dfc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Best_Model.h5')\n",
    "# model = keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7e01669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(train_dataset, epochs=10, validation_data=val_dataset) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "135ade1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_curves(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "797df0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_curves(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f74c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ارزیابی روی داده‌های اعتبارسنجی\n",
    "val_loss, val_accuracy = model.evaluate(val_dataset)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    directory = './test_data',\n",
    "    labels = None,\n",
    "    image_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    shuffle=False\n",
    ")\n",
    "test_dataset = test_dataset.map(lambda x: preprocess_input(x))\n",
    "\n",
    "test_predictions = model.predict(test_dataset)\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5c994d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load filenames (without .jpg)\n",
    "file_paths = glob.glob('test_data/*.jpg')\n",
    "file_names = [os.path.splitext(os.path.basename(f))[0] for f in file_paths]\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame()\n",
    "submission['file'] = file_names\n",
    "\n",
    "# Get predictions (assumes test_predictions already created)\n",
    "predicted_indices = np.argmax(test_predictions, axis=1)\n",
    "# predicted_indices → e.g., [0, 3, 2, 5, 1]\n",
    "# class_names → ['Andy_Warhol', 'Claude_Monet', ..., 'Vincent_van_Gogh']\n",
    "predicted_names = [df_train['painter'][i] for i in predicted_indices]\n",
    "# predicted_names → ['Andy Warhol', 'Claude Monet', ..., 'Vincent van Gogh']\n",
    "submission['artist'] = predicted_names  # ← ready to go\n",
    "\n",
    "print(submission)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
